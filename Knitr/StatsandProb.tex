\documentclass[12pt, a4paper, oneside]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt} % Paper size, default font size and one-sided paper
%\graphicspath{{./Figures/}} % Specifies the directory where pictures are stored
%\usepackage[dcucite]{harvard}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{pdflscape}
\usepackage{rotating}
\usepackage[flushleft]{threeparttable}
\usepackage{multirow}
\usepackage[comma, sort&compress]{natbib}% Use the natbib reference package - read up on this to edit the reference style; if you want text (e.g. Smith et al., 2012) for the in-text references (instead of numbers), remove 'numbers' 
\usepackage{graphicx}
%\bibliographystyle{plainnat}
\bibliographystyle{agsm}
\usepackage[colorlinks = true, citecolor = blue, linkcolor = blue]{hyperref}
%\hypersetup{urlcolor=blue, colorlinks=true} % Colors hyperlinks in blue - change to black if annoying
%\renewcommand[\harvardurl]{URL: \url}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\title{Stats and Probability Information}
%\author{Rob Hayward\footnote{University of Brighton Business School, Lewes Road, Brighton, BN2 4AT; Telephone 01273 642586.  rh49@brighton.ac.uk}}
\date{\today}
\maketitle
\subsection*{Continuous and marginal distributions}

The marginal distribution of $x$ in a two-variable distribution is equal to the sum of the joint distribution over $y$. 
\begin{equation}
Pr(X = x) = \sum_y Pr(X = x, Y = y) = \sum_y Pr(X = x|Y = y)Pr(Y = y)
\end{equation}

From \href{http://en.wikipedia.org/wiki/Marginal_distribution}{Wikipedia}

For the continuous case
\begin{equation}
p_X(x) = \int_y p_{X,Y}(x,y)dy = \int_y p_{X|Y}(x|y)p_Y(y)dy
\end{equation}

There are three related distributions:  the marginal, the joint and the conditional. 

\subsection{Markov Chain Monte Carlo Methods}
This comes from Dave Miles.  There are four sections. 
\begin{itemize}
\item \href{http://davegiles.blogspot.ca/2014/03/mcmc-for-econometrics-students-i.html}{Introduction}
\item \href{http://davegiles.blogspot.ca/2014/03/mcmc-for-econometrics-students-ii_18.html}{Showing the MCMC works}
\item \href{http://davegiles.blogspot.ca/2014/03/mcmc-for-econometrics-students-iii.html}{Example to extract the marginal posterior distribution}
\item \href{http://davegiles.blogspot.com/2014/03/mcmc-for-econometrics-students-part-iv.html#more}{Use R to implement MCMC}
\end{itemize}

This is an update of the code by \href{http://www.econometricsbysimulation.com/2014/04/dave-giles-on-mcmc-for-econometrics.html}{Economics by Simulation}

The Gibbs sampler exploits the characteristics of the Markov chain.  With two parameters $\theta_1$ and $\theta_2$, $p(\theta_1, \theta_2)$ is the prior pdf and $L(\theta_1, \theta_2 | y) = p(y | \theta_1, \theta_2$ is the likelihood function.  Using Bayes theory, the posterior pdf for the parameters is 
\begin{equation}
p(\theta_1, \theta_2| y) \propto p(\theta_1, \theta_2)L(\theta_1, \theta_2| y)
\end{equation}

There are a numbrer of steps. 
\begin{enumerate}
\item Assign initial values to $\theta_1^{(0)}$ and $\theta_2^{(0)}$
\item Draw a random value $\theta_1^{(1)}$ from $p(\theta_1|\theta_2^{(0)}, y)$
\item The draw a random value $\theta_2^{(1)}$ from $p(\theta_2|\theta_1^{(1)}, y)$
\item Draw a random value $\theta_1^{(2)}$ from $p(\theta_1|\theta_2^{(1)}, y)$
\item Repeat items 3 and 4
\end{enumerate}

We end up a with two series that are Markov chains so the initial values do not matter and after many replications the chains start to behave as if they were random draws from the \emph{marginal} posterior distibution $p(\theta_1 |y)$ and $p(\theta_2|y)$ rather than the \emph{conditional} posterior distribution $p(\theta_1| \theta_2, y)$ $p(\theta_1|\theta_2, y)$.  The early values are part of the \emph{burn in} period and should be discarded.   

Here is a very simple version of the sampler applied to linear regression.  A draw for the slope coefficient is made conditional upon the given intercept and then a new intercept is drawn conditional on the slope.  This is repeated.  I think that they should be random draws from the data.  

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Create a simple MCMC sampler}
\hlstd{x} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{100}\hlstd{,} \hlkwc{by} \hlstd{=} \hlnum{1}\hlstd{)}
\hlstd{e} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlkwd{length}\hlstd{(x))}
\hlstd{y} \hlkwb{<-} \hlnum{0.5} \hlopt{+} \hlnum{5} \hlopt{*} \hlstd{x} \hlopt{+} \hlstd{e}
\hlcom{# prior}
\hlstd{a} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwc{times} \hlstd{=} \hlkwd{length}\hlstd{(x))}
\hlstd{b} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwc{times} \hlstd{=} \hlkwd{length}\hlstd{(x))}
\hlstd{a[}\hlnum{1}\hlstd{]} \hlkwb{=} \hlnum{1}
\hlstd{b[}\hlnum{1}\hlstd{]} \hlkwb{=} \hlnum{1}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{2}\hlopt{:}\hlnum{100}\hlstd{) \{}
    \hlstd{b[i]} \hlkwb{<-} \hlstd{(y[i]} \hlopt{-} \hlstd{a[i} \hlopt{-} \hlnum{1}\hlstd{]} \hlopt{-} \hlkwd{rnorm}\hlstd{(}\hlnum{1}\hlstd{))}\hlopt{/}\hlstd{x[i]}
    \hlstd{a[i]} \hlkwb{<-} \hlstd{y[i]} \hlopt{-} \hlstd{b[i]} \hlopt{*} \hlstd{x[i]} \hlopt{-} \hlkwd{rnorm}\hlstd{(}\hlnum{1}\hlstd{)}
\hlstd{\}}
\hlkwd{plot}\hlstd{(y} \hlopt{~} \hlstd{x,} \hlkwc{type} \hlstd{=} \hlstr{"p"}\hlstd{,} \hlkwc{main} \hlstd{=} \hlstr{"Regression with OLS (red) and MCMC (blue)"}\hlstd{)}
\hlkwd{abline}\hlstd{(}\hlkwc{a} \hlstd{=} \hlkwd{mean}\hlstd{(a),} \hlkwc{b} \hlstd{=} \hlkwd{mean}\hlstd{(b),} \hlkwc{col} \hlstd{=} \hlstr{"blue"}\hlstd{)}
\hlkwd{lines}\hlstd{(}\hlkwd{fitted}\hlstd{(}\hlkwd{lm}\hlstd{(y} \hlopt{~} \hlstd{x)),} \hlkwc{col} \hlstd{=} \hlstr{"red"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/MCMC} 

\end{knitrout}


Once there is a large sample of $p(\theta_1|y)$ and the burn-in have been discarded, the mean, variance, median or mode of the marginal posterior can be calculated.  The process can be expanded to more parameters. 

\subsubsection{Gibbs Sampler}
The Gibbs sampler is a special case of the Metropolis-Hasting algorithm. 
The R code from Dave Giles in the file DaveGilesMCMCcoes.R has better version than this. I am not sure how we got from $\rho = 0.5$ to $sd = 1-\rho^2$. 

This example is based on two random variables $Y_1$ and $Y_2$, with a mean vector of $(\mu_1, \mu_2)$ a correlation of $\rho$, the variances of $Y_1$ and $Y_2$ are $\sigma_1^2$ and $\sigma_2^2$ respectively and the covariance between $Y_1$ and $Y_2$ is $\rho \sigma_1 \sigma_2$. \footnote{As $\rho = \frac{\sigma_{Y_1, Y_2}}{\sigma_1 \sigma_2}$.  Some additional reading on covariance \href{http://www.cogsci.ucsd.edu/~desa/109/trieschmarksslides.pdf}{http://www.cogsci.ucsd.edu/~desa/109/trieschmarksslides.pdf}}



The conditional distribution of $Y_1$ given $Y_2$ is 

\begin{equation}
p(Y_1| Y_2) \sim N \left(\mu_1 + \frac{\rho\sigma_1(Y_2 - \mu_2)}{\sigma_2}, \sigma_1^2(1 - \rho^2)\right)
\end{equation}

and the conditional distribution of $Y_2$ given $Y_1$ is

\begin{equation}
p(Y_2| Y_1) \sim N \left(\mu_2 + \frac{\rho\sigma_2(Y_1 - \mu_1)}{\sigma_2}, \sigma_1^2(1 - \rho^2) \right)
\end{equation}

This can be used to find the marginal distribution. It is known that the marginal distribution for $Y_1$ is
\begin{equation}
p(Y_1) \sim N(\mu_1, \sigma_1^2)
\end{equation}

and for $Y_2$ is 
\begin{equation}
p(Y_2) \sim N(\mu_2, \sigma_2^2)
\end{equation}

However, to test the MCMC draw from the conditional to find the marginal (that we already know).  

Set $\mu_1 = 0$ and $\mu_2 = 0$ and $\sigma_1 = 1$ and $\sigma_2 = 2$

\subsection{Mixture Model}
This is a probabilistic model that relates some random variables to some other variables.  The model has sub-populations. The properties of the sub-population are different from those of the parent. The sub-populations may not be observable.  For example, the distribution of returns may be different in different sub-population or regime. 

A \emph{mixture distribution} is the probability distribution of a random variable  whose values are derived from an underlying set of random variables. The \emph{mixture components} are individual distributions with \emph{mixture weights}.  Even in cases where the mixture comonents have a normal distribution, the mixture distribution is likely to be non-normal. Mixture models are used to understand the sub-population when there is only access to the information about the pooled population. 

The mixture model will be comprised of N random varibles distributed according to K components, with each component belonging to the same distribution. The k mixture weights sum to one. Each component will have parameters (mean and variance in the case of normal distribution).  

The method will try to estimate the all the parameters of the model from the data.  The underlying data is known $(x_i)$; the number of mixture components is set $(K)$; the parameters of the distribution of each mixture component $(\theta_{i=1\dots K})$; mixture weight $(\Phi_{i = 1\dots K})$; $\mathbf{\Phi}$ K-dimensional vector summing to 1; $F(x|\theta)$ probability distribution of observations parameterised on $\theta$; $\alpha$ shared hyperparameter for component weights; $\beta$ shared hyperparameter for mixture weights; $H(\theta|\alpha)$ prior probability distribution of component parameters; 

\subsection{Adjusted R squared}
\href{http://davegiles.blogspot.ca/2013/05/when-will-adjusted-r-squared-increase.html}{Adjusted R squared} applied a penalty to the basic R squard to account for additional variables.  The equartion is 

\begin{equation}
R_A^2 = 1 - \left [ \frac{(n-1)}{(n-k)} \right ] [1 - R^2]
\end{equation}

Adding a regressor to the equation will increase (reduce)) the $R_A^"$ when the absolute value of the t-statistic is greater (less) than one. Adding a group of regressors to the model will reduce (increase) the $R_A^"$ when the absolute value of the F-statistic is greater than one.  

Proof \href{http://davegiles.blogspot.com/2014/04/proof-of-result-about-adjusted.html}{http://davegiles.blogspot.com/2014/04/proof-of-result-about-adjusted.html}

\subsection{Monte Carlo Simulation}
This comes from \href{http://blog.revolutionanalytics.com/2014/04/quantitative-finance-applications-in-r-5.html}{Revoluitionary Analytics}.  The analysis is in annual terms.  

\begin{equation}
\mu \Delta t + \sigma Z \sqrt{\Delta t}
\end{equation}

where $\mu$ is the drift or average annual return, Z is a standard Normal random variable, t is measured in years so for monthly returns $\Delta t$ equals $\frac{1}{12}$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{n} \hlkwb{<-} \hlnum{10000}
\hlcom{# Fixing the seed gives us a consistent set of simulated returns}
\hlkwd{set.seed}\hlstd{(}\hlnum{106}\hlstd{)}
\hlstd{z} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(n)}  \hlcom{# mean = 0 and sd = 1 are defaults}
\hlstd{mu} \hlkwb{<-} \hlnum{0.1}
\hlstd{sd} \hlkwb{<-} \hlnum{0.15}
\hlstd{delta_t} \hlkwb{<-} \hlnum{0.25}
\hlcom{# apply to expression (*) above}
\hlstd{qtr_returns} \hlkwb{<-} \hlstd{mu} \hlopt{*} \hlstd{delta_t} \hlopt{+} \hlstd{sd} \hlopt{*} \hlstd{z} \hlopt{*} \hlkwd{sqrt}\hlstd{(delta_t)}
\hlkwd{hist}\hlstd{(qtr_returns,} \hlkwc{breaks} \hlstd{=} \hlnum{100}\hlstd{,} \hlkwc{col} \hlstd{=} \hlstr{"green"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/MC} 

\end{knitrout}


Now the descriptive statistics can be uncovered from the simulated results. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{stats} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{mean}\hlstd{(qtr_returns)} \hlopt{*} \hlnum{4}\hlstd{,} \hlkwd{sd}\hlstd{(qtr_returns)} \hlopt{*} \hlnum{2}\hlstd{)}  \hlcom{# sqrt(4)}
\hlkwd{names}\hlstd{(stats)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"mean"}\hlstd{,} \hlstr{"volatility"}\hlstd{)}
\hlstd{stats}
\end{alltt}
\begin{verbatim}
##       mean volatility 
##    0.09901    0.14976
\end{verbatim}
\end{kframe}
\end{knitrout}

This is the basic model.  It would also be possible to simulate two variables and to include some relationship between the two in the analysis.  It would also be possible to simulate an asset in two different regimes. A Monte-Carlo Markov Model (MCMM) would require another set of $\mu$ and $\sigma$ inputs as well as a transition matrix of the probabilities that there is a switch from one regime to another. 
 
\subsection{Generalised Lambda Distribution}
This is from \href{http://blog.revolutionanalytics.com/2014/02/quantitative-finance-applications-in-r-4-using-the-generalized-lambda-distribution-to-simulate-market-returns.html}{Revolutionary Analytics}.  The four parameters $\lambda_1$, $\lambda_2$, $\lambda_3$ and $\lambda_4$ indicate the location, scale, skew and kurtosis of the distribution. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{require}\hlstd{(GLDEX)}
\hlkwd{require}\hlstd{(quantmod)}
\hlkwd{getSymbols}\hlstd{(}\hlstr{"SPY"}\hlstd{,} \hlkwc{from} \hlstd{=} \hlstr{"1994-02-01"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] "SPY"
\end{verbatim}
\begin{alltt}
\hlstd{SPY.Close} \hlkwb{<-} \hlstd{SPY[,} \hlnum{4}\hlstd{]}  \hlcom{# Closing prices}
\hlstd{SPY.vector} \hlkwb{<-} \hlkwd{as.vector}\hlstd{(SPY.Close)}
\hlcom{# Calculate log returns}
\hlstd{sp500} \hlkwb{<-} \hlkwd{diff}\hlstd{(}\hlkwd{log}\hlstd{(SPY.vector),} \hlkwc{lag} \hlstd{=} \hlnum{1}\hlstd{)}
\hlstd{sp500} \hlkwb{<-} \hlstd{sp500[}\hlopt{-}\hlnum{1}\hlstd{]}  \hlcom{# Remove the NA in the first position}
\hlcom{# Set normalise='Y' so that kurtosis is calculated with reference to}
\hlcom{# kurtosis = 0 under Normal distribution}
\hlkwd{fun.moments.r}\hlstd{(sp500,} \hlkwc{normalise} \hlstd{=} \hlstr{"Y"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##       mean   variance   skewness   kurtosis 
##  0.0002633  0.0001532 -0.0960976  9.5131553
\end{verbatim}
\end{kframe}
\end{knitrout}

Now fit the GLD with the function fun.data.fit.mm. There are warnings but these can be ignored. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{spLambdaDist} \hlkwb{=} \hlkwd{fun.data.fit.mm}\hlstd{(sp500)}
\hlstd{spLambdaDist}
\end{alltt}
\begin{verbatim}
##            RPRS     RMFMKL
## [1,]  3.846e-04   0.000321
## [2,] -4.228e+01 203.501581
## [3,] -1.675e-01  -0.169657
## [4,] -1.640e-01  -0.161483
\end{verbatim}
\begin{alltt}
\hlkwd{fun.plot.fit}\hlstd{(}\hlkwc{fit.obj} \hlstd{= spLambdaDist,} \hlkwc{data} \hlstd{= sp500,} \hlkwc{nclass} \hlstd{=} \hlnum{100}\hlstd{,} \hlkwc{param} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"rs"}\hlstd{,}
    \hlstr{"fmkl"}\hlstd{),} \hlkwc{xlab} \hlstd{=} \hlstr{"Returns"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/GLD} 

\end{knitrout}

Now it is possible to generrate simualted results using the function rgl(). Lambdas need to be identified. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{lambda_params_rs} \hlkwb{<-} \hlstd{spLambdaDist[,} \hlnum{1}\hlstd{]}
\hlstd{lambda1_rs} \hlkwb{<-} \hlstd{lambda_params_rs[}\hlnum{1}\hlstd{]}
\hlstd{lambda2_rs} \hlkwb{<-} \hlstd{lambda_params_rs[}\hlnum{2}\hlstd{]}
\hlstd{lambda3_rs} \hlkwb{<-} \hlstd{lambda_params_rs[}\hlnum{3}\hlstd{]}
\hlstd{lambda4_rs} \hlkwb{<-} \hlstd{lambda_params_rs[}\hlnum{4}\hlstd{]}
\hlstd{lambda_params_fmkl} \hlkwb{<-} \hlstd{spLambdaDist[,} \hlnum{2}\hlstd{]}
\hlstd{lambda1_fmkl} \hlkwb{<-} \hlstd{lambda_params_fmkl[}\hlnum{1}\hlstd{]}
\hlstd{lambda2_fmkl} \hlkwb{<-} \hlstd{lambda_params_fmkl[}\hlnum{2}\hlstd{]}
\hlstd{lambda3_fmkl} \hlkwb{<-} \hlstd{lambda_params_fmkl[}\hlnum{3}\hlstd{]}
\hlstd{lambda4_fmkl} \hlkwb{<-} \hlstd{lambda_params_fmkl[}\hlnum{4}\hlstd{]}
\end{alltt}
\end{kframe}
\end{knitrout}

Now generate simulations of each variety. 

There are problems with the rgl function.  I am not sure what this does.  It is 10 million simulations.  I think that the rgl just uses extra hardware to make the change. It may be useful to re-do this last section using a different method. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{require}\hlstd{(gld)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: gld}}\begin{alltt}
\hlkwd{require}\hlstd{(GLDEX)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: GLDEX\\\#\# Loading required package: cluster\\\#\# \\\#\# Attaching package: 'GLDEX'\\\#\# \\\#\# The following objects are masked from 'package:gld':\\\#\# \\\#\#\ \ \ \  dgl, pgl, qdgl, qgl, rgl, starship, starship.adaptivegrid,\\\#\#\ \ \ \  starship.obj}}\begin{alltt}
\hlcom{# RS version:}
\hlkwd{set.seed}\hlstd{(}\hlnum{100}\hlstd{)}  \hlcom{# Set seed to obtain a reproducible set}
\hlstd{rs_sample} \hlkwb{<-} \hlkwd{rgl}\hlstd{(}\hlkwc{n} \hlstd{=} \hlnum{1e+07}\hlstd{,} \hlkwc{lambda1} \hlstd{= lambda1_rs,} \hlkwc{lambda2} \hlstd{= lambda2_rs,} \hlkwc{lambda3} \hlstd{= lambda3_rs,}
    \hlkwc{lambda4} \hlstd{= lambda4_rs,} \hlkwc{param} \hlstd{=} \hlstr{"rs"}\hlstd{)}
\hlcom{# Moments of simulated returns using RS method:}
\hlkwd{fun.moments.r}\hlstd{(rs_sample,} \hlkwc{normalise} \hlstd{=} \hlstr{"Y"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##       mean   variance   skewness   kurtosis 
##  2.633e-04  9.774e-05 -1.043e-01  9.955e+00
\end{verbatim}
\begin{alltt}
\hlcom{# Moments calculated from market data:}
\hlkwd{fun.moments.r}\hlstd{(sp500,} \hlkwc{normalise} \hlstd{=} \hlstr{"Y"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##       mean   variance   skewness   kurtosis 
##  0.0002633  0.0001532 -0.0960976  9.5131553
\end{verbatim}
\begin{alltt}
\hlcom{# FKML version:}
\hlkwd{set.seed}\hlstd{(}\hlnum{100}\hlstd{)}  \hlcom{# Set seed to obtain a reproducible set}
\hlstd{fmkl_sample} \hlkwb{<-} \hlkwd{rgl}\hlstd{(}\hlkwc{n} \hlstd{=} \hlnum{1e+05}\hlstd{,} \hlkwc{lambda1} \hlstd{= lambda1_fmkl,} \hlkwc{lambda2} \hlstd{= lambda2_fmkl,}
    \hlkwc{lambda3} \hlstd{= lambda3_fmkl,} \hlkwc{lambda4} \hlstd{= lambda4_fmkl,} \hlkwc{param} \hlstd{=} \hlstr{"fmkl"}\hlstd{)}
\hlcom{# Moments of simulated returns using FMKL method:}
\hlkwd{fun.moments.r}\hlstd{(fmkl_sample,} \hlkwc{normalise} \hlstd{=} \hlstr{"Y"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##       mean   variance   skewness   kurtosis 
##  0.0002403  0.0001547 -0.0862244  8.5839659
\end{verbatim}
\begin{alltt}
\hlcom{# Moments calculated from market data:}
\hlkwd{fun.moments.r}\hlstd{(sp500,} \hlkwc{normalise} \hlstd{=} \hlstr{"Y"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##       mean   variance   skewness   kurtosis 
##  0.0002633  0.0001532 -0.0960976  9.5131553
\end{verbatim}
\end{kframe}
\end{knitrout}

Compare the moments to the S\&P500 market data
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{fun.moments.r}\hlstd{(rs_sample,} \hlkwc{normalise} \hlstd{=} \hlstr{"Y"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##       mean   variance   skewness   kurtosis 
##  2.633e-04  9.774e-05 -1.043e-01  9.955e+00
\end{verbatim}
\begin{alltt}
\hlkwd{fun.moments.r}\hlstd{(sp500,} \hlkwc{normalise} \hlstd{=} \hlstr{"Y"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##       mean   variance   skewness   kurtosis 
##  0.0002633  0.0001532 -0.0960976  9.5131553
\end{verbatim}
\begin{alltt}
\hlkwd{fun.moments.r}\hlstd{(fmkl_sample,} \hlkwc{normalise} \hlstd{=} \hlstr{"Y"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##       mean   variance   skewness   kurtosis 
##  0.0002403  0.0001547 -0.0862244  8.5839659
\end{verbatim}
\begin{alltt}
\hlkwd{fun.moments.r}\hlstd{(sp500,} \hlkwc{normalise} \hlstd{=} \hlstr{"Y"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##       mean   variance   skewness   kurtosis 
##  0.0002633  0.0001532 -0.0960976  9.5131553
\end{verbatim}
\end{kframe}
\end{knitrout}

\subsection{Lasso method}
\href{http://www.mathtube.org/lecture/video/lasso-brief-review-and-new-significance-test#.U1aODV1sp94.twitter}{Rob Tibshirani}Cancer example that requires identification of appropriate cell. There are 20 cases that are being used as a training set.  Train a classifier to identify whether the cells are cencerous or not.  There are 11,000 features. It would be useful to use as few of the features as possible. Therefore, also want to know which features are important for the classification. 

Sparcity means that the features are reduced by only using those that pass a particular level of significance.  \href{http://statweb.stanford.edu/~tibs/lasso.html}{More here.}

\subsection{Standard Error of the estimated mean}
The standard error of the estimate of the mean is 
\begin{equation}
SD_x = \frac{\sigma}{\sqrt{n}}
\end{equation}
This can be derived from the variance of the sum of independent random variables
\begin{itemize}
\item If $X_1, X_2, \dots, X_n$ are independent observations from a population with a mean $\mu$ and a standard deviation $\sigma$, 
\item Variance of the Total = $T = (X_1, X_2, \dots, X_n)$ is $n\sigma^2$
\item $T/n$ is the mean $\bar{x}$
\item the variance of $T/n$ is $\frac{1}{n^2}n\sigma^2 = \frac{\sigma^2}{n}$
\item \href{https://onlinecourses.science.psu.edu/stat414/node/167}{Explained}
\item the standard deviation of $T/n$ must be $\frac{n}{\sqrt{n}}$
\end{itemize}

\subsection{Logistic Regression}
This comes from \href{http://www.win-vector.com/blog/2010/11/learn-a-powerful-machine-learning-tool-logistic-regression-and-beyond/}{Wim-Vector - logistic regression}.  

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{CarData} \hlkwb{<-} \hlkwd{read.table}\hlstd{(}\hlkwd{url}\hlstd{(}\hlstr{"http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data"}\hlstd{),}
    \hlkwc{sep} \hlstd{=} \hlstr{","}\hlstd{,} \hlkwc{col.names} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"buying"}\hlstd{,} \hlstr{"maintenance"}\hlstd{,} \hlstr{"doors"}\hlstd{,} \hlstr{"persons"}\hlstd{,} \hlstr{"lug_boot"}\hlstd{,}
        \hlstr{"safety"}\hlstd{,} \hlstr{"rating"}\hlstd{))}
\hlstd{logisticModel} \hlkwb{<-} \hlkwd{glm}\hlstd{(rating} \hlopt{!=} \hlstr{"unacc"} \hlopt{~} \hlstd{buying} \hlopt{+} \hlstd{maintenance} \hlopt{+} \hlstd{doors} \hlopt{+} \hlstd{persons} \hlopt{+}
    \hlstd{lug_boot} \hlopt{+} \hlstd{safety,} \hlkwc{family} \hlstd{=} \hlkwd{binomial}\hlstd{(}\hlkwc{link} \hlstd{=} \hlstr{"logit"}\hlstd{),} \hlkwc{data} \hlstd{= CarData)}
\end{alltt}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred}}\begin{alltt}
\hlkwd{summary}\hlstd{(logisticModel)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## glm(formula = rating != "unacc" ~ buying + maintenance + doors + 
##     persons + lug_boot + safety, family = binomial(link = "logit"), 
##     data = CarData)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -3.216   0.000   0.000   0.026   2.434  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(>|z|)    
## (Intercept)       -28.426   1257.526   -0.02     0.98    
## buyinglow           5.048      0.567    8.90  < 2e-16 ***
## buyingmed           3.922      0.484    8.10  5.5e-16 ***
## buyingvhigh        -2.066      0.375   -5.51  3.5e-08 ***
## maintenancelow      3.406      0.469    7.26  3.9e-13 ***
## maintenancemed      3.406      0.469    7.26  3.9e-13 ***
## maintenancevhigh   -2.825      0.415   -6.82  9.4e-12 ***
## doors3              1.856      0.404    4.59  4.4e-06 ***
## doors4              2.482      0.428    5.80  6.6e-09 ***
## doors5more          2.482      0.428    5.80  6.6e-09 ***
## persons4           29.965   1257.526    0.02     0.98    
## personsmore        29.584   1257.526    0.02     0.98    
## lug_bootmed        -1.517      0.376   -4.04  5.4e-05 ***
## lug_bootsmall      -4.448      0.475   -9.36  < 2e-16 ***
## safetylow         -30.505   1300.343   -0.02     0.98    
## safetymed          -3.004      0.358   -8.40  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2110.47  on 1727  degrees of freedom
## Residual deviance:  339.36  on 1712  degrees of freedom
## AIC: 371.4
## 
## Number of Fisher Scoring iterations: 21
\end{verbatim}
\end{kframe}
\end{knitrout}

The variable levels and values are joined together. The model will provide an estimate of the effect of each category on the rating.  The values for each category are adde together to get the overall rating score. 

The error results of the model can be examined. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{table}\hlstd{(CarData}\hlopt{$}\hlstd{rating,} \hlkwd{predict}\hlstd{(logisticModel,} \hlkwc{type} \hlstd{=} \hlstr{"response"}\hlstd{)} \hlopt{>=} \hlnum{0.5}\hlstd{)}
\end{alltt}
\begin{verbatim}
##        
##         FALSE TRUE
##   acc      32  352
##   good      0   69
##   unacc  1166   44
##   vgood     0   65
\end{verbatim}
\end{kframe}
\end{knitrout}


To be completed. 

\subsection{F-Tests}
This is an example that is based on testing for fabricated evidence.  The full documentation is \href{http://errorstatistics.com/2014/05/10/who-ya-gonna-call-for-statistical-fraudbusting-r-a-fisher-p-values-and-error-statistics-again/}{Deborah Mayo}.  As the paper says
\begin{quotation}
``In each experiment, participants (undergraduate students) were randomly assigned to three groups, and each group was given a different intervention. All participants were then tested on some outcome measure.``
\end{quotation}
The accusation is that the results are \textbf{too} linear.  The relationships are too perfect compared to other studies.  The method used to asses this is called \emph{delta-F}.  The odds of seeing such linear trends are calculated based on the assumption that the trend is linear.  

Unless there is a huge sample, the probability of obtaining a linear trend is very low becuase there is noise. THe amount of noise is evident in the \emph{within group variance}.  For a given sample size and a given level of within group variance, the odds of obtaining a linear trend are calculated as the sum of squares accounted for by a linear model and a none-linear model (one-way ANOVA) divided by the mean-square error (within group variance).

\begin{equation}
\Delta F = \frac{SS_{REG} - SS_B}{MS_W}
\end{equation}

There is one degree of freedom in the numerator and $3(n-1)$ degrees of freedom in the denominator. 

If the difference between the two models (linear and non-linear) is small, it means that there is an underlying linear relationship. Assuming that the relationship is linear, this delta-F metric should follow an F distribution. 

This is more or less the same method that is used to test whether a simple model fits the data as well as a complex model. In that case, the null hypothesis is that the simple model is the correct version and the objective is to determine if the difference between the two is unlikely given the null. 

From the paper

\begin{quotation}
```But here the whole thing is turned on its head. Random noise means that a complex model will sometimes fit the data better than a simple one, even if the simple model describes reality. In a conventional use of F-tests, that would be regarded as a false positive. But in this case its the absence of those false positives thats unusual.```
\end{quotation}

\end{document}
